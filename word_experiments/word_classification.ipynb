{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import smart_open\n",
    "smart_open.open = smart_open.smart_open\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from random import shuffle\n",
    "from collections import defaultdict\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_definitional_pairs_100 = pd.read_excel(\"../data/100_wordpairs.xlsx\")\n",
    "definitional_pairs_100 = list(zip(df_definitional_pairs_100.Profan.tolist(), df_definitional_pairs_100.Neutral.tolist()))\n",
    "test_words = pd.read_excel(\"../data/test_words.xlsx\")\n",
    "human_nouns = test_words.nouns_human.tolist()\n",
    "general_nouns = test_words.nouns_general.tolist()\n",
    "verbs_adjs = test_words.verbs_adjectives.tolist()\n",
    "test_labels = ([\"OFF\"] * 25) + ([\"OTH\"] * 25)\n",
    "model = KeyedVectors.load_word2vec_format('../data/embed_tweets_de_300D_fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_LDA(pairs, embedding, test_words, test_labels, normalize=False, num_components = 10):\n",
    "    matrix = []\n",
    "    word_labels = []\n",
    "    for a, b in pairs:\n",
    "        if normalize:\n",
    "            center = (embedding[a] + embedding[b])/2\n",
    "            matrix.append(embedding[a] - center)\n",
    "            matrix.append(embedding[b] - center)\n",
    "        else:\n",
    "            matrix.append(embedding[a])\n",
    "            matrix.append(embedding[b])\n",
    "        word_labels.append(\"OFF\")\n",
    "        word_labels.append(\"OTH\")\n",
    "    matrix = np.array(matrix)\n",
    "    pca = PCA(n_components = num_components)\n",
    "\n",
    "    results = {}\n",
    "    pca_results = pca.fit_transform(matrix)\n",
    "    lda = LDA(n_components=1)\n",
    "    lda.fit(pca_results, word_labels)\n",
    "    \n",
    "    test_words_embeddings = np.array([embedding[w] for w in test_words])\n",
    "    pca_results_test = pca.transform(test_words_embeddings)\n",
    "    predictions = lda.predict(pca_results_test)\n",
    "\n",
    "    results['cm'] = confusion_matrix(y_true=arab_labels,y_pred=predictions)\n",
    "    results['f1_micro'] = f1_score(y_true=test_labels,y_pred=predictions,average='micro')\n",
    "    results['f1_macro'] = f1_score(y_true=test_labels,y_pred=predictions,average='macro')\n",
    "    return results[\"f1_macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doLDA(pairs, embedding, test_words, test_labels):\n",
    "    matrix = []\n",
    "    word_labels = []\n",
    "    for a, b in pairs:\n",
    "        matrix.append(embedding[a])\n",
    "        matrix.append(embedding[b])\n",
    "        word_labels.append(\"OFF\")\n",
    "        word_labels.append(\"OTH\")\n",
    "    matrix = np.array(matrix)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    lda = LDA(n_components=1)\n",
    "    lda.fit(matrix, word_labels)\n",
    "    \n",
    "    test_words_embeddings = np.array([embedding[w] for w in test_words])\n",
    "    predictions = lda.predict(test_words_embeddings)\n",
    "    \n",
    "    results['cm'] = confusion_matrix(y_true=arab_labels,y_pred=predictions)\n",
    "    results['f1_micro'] = f1_score(y_true=test_labels,y_pred=predictions,average='micro')\n",
    "    results['f1_macro'] = f1_score(y_true=test_labels,y_pred=predictions,average='macro')\n",
    "\n",
    "    return results[\"f1_macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Classification Experiment: Decreasing Word Training Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on PCA-LDA: Find best number of PCs\n",
    "best_pcs = defaultdict()\n",
    "for step_size in range(10,101,10): #increase number of word pairs by 10\n",
    "    pc_results = defaultdict(list)\n",
    "    for i in range(10): #10-fold\n",
    "        shuffle(definitional_pairs_100)\n",
    "        sample = definitional_pairs_100[:step_size]\n",
    "        sample_test = list(chain(*definitional_pairs_100[-10:]))\n",
    "        sample_test_labels = [\"OFF\", \"OTH\"] * 10\n",
    "\n",
    "        for pc in range(1, len(sample)*2): #for each possible number of PCs\n",
    "            f1 = PCA_LDA(sample, model, sample_test, sample_test_labels, normalize=False,num_components=pc)\n",
    "            pc_results[pc].append(f1)\n",
    "    best_folds = []\n",
    "    for k,v in pc_results.items():\n",
    "        #print(k, max(v), np.std(v))\n",
    "        best_folds.append(max(v) - np.std(v))\n",
    "    best_pcs[step_size] = best_folds.index(max(best_folds)) + 1\n",
    "print(best_pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_human, results_noun, results_verb = [],[],[]\n",
    "for step_size in range(10,101,10): #increase number of word pairs by 10\n",
    "    kfold_results_human, kfold_results_noun, kfold_results_verb = [],[],[]\n",
    "    for i in range(10): #10-fold\n",
    "        shuffle(definitional_pairs_100)\n",
    "        sample = definitional_pairs_100[:step_size+10]\n",
    "        f1_human = PCA_LDA(sample, model, human_nouns, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_human.append(f1_human)\n",
    "        f1_noun = PCA_LDA(sample, model, general_nouns, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_noun.append(f1_noun)\n",
    "        f1_verb = PCA_LDA(sample, model, verbs_adjs, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_verb.append(f1_verb)\n",
    "    results_human.append(np.mean(kfold_results_human))\n",
    "    results_noun.append(np.mean(kfold_results_noun))\n",
    "    results_verb.append(np.mean(kfold_results_verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on PCA-LDA-NORM\n",
    "best_pcs = defaultdict()\n",
    "for step_size in range(10,101,10): #increase number of word pairs by 10\n",
    "    pc_results = defaultdict(list)\n",
    "    for i in range(10): #10-fold\n",
    "        shuffle(definitional_pairs_100)\n",
    "        sample = definitional_pairs_100[:step_size]\n",
    "        sample_test = list(chain(*definitional_pairs_100[-10:]))\n",
    "        sample_test_labels = [\"OFF\", \"OTH\"] * 10\n",
    "        for pc in range(1, len(sample)*2): #for each possible number of PCs\n",
    "            f1 = PCA_LDA(sample, model, sample_test, sample_test_labels, normalize=True,num_components=pc)\n",
    "            pc_results[pc].append(f1)\n",
    "    best_folds = []\n",
    "    for k,v in pc_results.items():\n",
    "        #print(k, max(v), np.std(v))\n",
    "        best_folds.append(max(v) - np.std(v))\n",
    "    best_pcs[step_size] = best_folds.index(max(best_folds)) + 1\n",
    "print(best_pcs)\n",
    "\n",
    "results_human_norm, results_noun_norm, results_verb_norm = [],[],[]\n",
    "for step_size in range(10,101,10): #increase number of word pairs by 10\n",
    "    kfold_results_human, kfold_results_noun, kfold_results_verb = [],[],[]\n",
    "    for i in range(10): #10-fold\n",
    "        shuffle(definitional_pairs_100)\n",
    "        sample = definitional_pairs_100[:step_size+10]\n",
    "        f1_human = PCA_LDA(sample, model, human_nouns, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_human.append(f1_human)\n",
    "        f1_noun = PCA_LDA(sample, model, general_nouns, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_noun.append(f1_noun)\n",
    "        f1_verb = PCA_LDA(sample, model, verbs_adjs, test_labels, normalize=False,num_components=best_pcs[step_size])\n",
    "        kfold_results_verb.append(f1_verb)\n",
    "    results_human_norm.append(np.mean(kfold_results_human))\n",
    "    results_noun_norm.append(np.mean(kfold_results_noun))\n",
    "    results_verb_norm.append(np.mean(kfold_results_verb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on LDA\n",
    "results_human_LDA, results_noun_LDA, results_verb_LDA = [],[],[]\n",
    "for step_size in range(10,101,10): #increase number of word pairs by 10\n",
    "    kfold_results_human, kfold_results_noun, kfold_results_verb = [],[],[]\n",
    "    for i in range(10): #10-fold\n",
    "        shuffle(definitional_pairs_100)\n",
    "        sample = definitional_pairs_100[:step_size+10]\n",
    "        f1_human = doLDA(sample, model, human_nouns, test_labels)\n",
    "        kfold_results_human.append(f1_human)\n",
    "        f1_noun = doLDA(sample, model, general_nouns, test_labels)\n",
    "        kfold_results_noun.append(f1_noun)\n",
    "        f1_verb = doLDA(sample, model, verbs_adjs, test_labels)\n",
    "        kfold_results_verb.append(f1_verb)\n",
    "    results_human_LDA.append(np.mean(kfold_results_human))\n",
    "    results_noun_LDA.append(np.mean(kfold_results_noun))\n",
    "    results_verb_LDA.append(np.mean(kfold_results_verb))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
